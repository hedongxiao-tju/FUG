{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a399ecd-d213-4154-a8bd-6168d42df207",
   "metadata": {},
   "source": [
    "# This is an example for easily using FUG on your datasets\n",
    "\n",
    "Firstly, please check your environment. The necessary libraries are torch and torch_geometric.   \n",
    "Please note that, in this folder, the 'Utils' file is simplified for ease of use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708005e6-57bd-4d39-9358-6040c7affefa",
   "metadata": {},
   "source": [
    "# The following is an example to show how to create a new FUG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7944f9f-c472-4a7d-aed6-1d5f16364e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch_geometric as pyg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch\n",
    "import torch_geometric\n",
    "# Please make sure you import the following lib from correct file. \n",
    "# You can change the lib name or the location of 'FUGNN' and 'Utils'  as long as they are imported correctly.\n",
    "from FUGNN import DimensionNN_V2, GCN_encoder, FUG\n",
    "from Utils import dimensional_sample_random, DAD_edge_index, get_embedding\n",
    "\n",
    "# The following hyper-parameters can be changed if you want to train your FUG.\n",
    "# The default hyper-parameters are the same with the FUG saved in './pt/Trained_on_all_7_datasets.pt'\n",
    "activator = nn.PReLU\n",
    "sample_size = 1024\n",
    "feature_signal_dim = 1024\n",
    "hid_units = 1024\n",
    "if_rand = False\n",
    "    \n",
    "dnn = DimensionNN_V2(sample_size, feature_signal_dim*2, feature_signal_dim, activator)\n",
    "gnn = GCN_encoder(feature_signal_dim, hid_units, activator)\n",
    "model = FUG(D_NN=dnn, G_NN=gnn, S_mtd=dimensional_sample_random, sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc3daa-197a-4b88-ad5a-2c418917d84b",
   "metadata": {},
   "source": [
    "# How to load our FUG model (pre-trained on Cora, Citeseer, Pubmed, Photo, Computers, CS and Physics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09743814-d7d8-4652-aa7a-08ab5e2ef9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# the location of model\n",
    "model_dir_gpu = './pt/Trained_on_all_7_datasets.pt'\n",
    "model_dir_cpu = './pt/Trained_on_all_7_datasets_cpu.pt'\n",
    "\n",
    "model = torch.load(model_dir_gpu) if torch.cuda.is_available() else torch.load(model_dir_cpu).eval()\n",
    "# Trained_on_all_7_datasets.pt and Trained_on_all_7_datasets_cpu.pt can be found in https://github.com/slz1024/FUG_Large/tree/master.
     # Actrually you can easily change the device by 'model = model.cuda()' or 'model = model.cpu()'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f27442-868a-4b05-a661-176b8912d696",
   "metadata": {},
   "source": [
    "# How to generate embeddings by a trained FUG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca5e004-a9b4-4635-a59c-5492f1ad91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######---Data----##########\n",
      "Dataloader: Loading Dataset Cora\n",
      "Dataloader: Loading success.\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "#####--Embeddings--#########\n",
      "tensor([[-8.0686e-05, -1.3166e-03,  1.7949e-03,  ..., -2.9779e-03,\n",
      "          7.7205e-03, -1.7718e-03],\n",
      "        [ 7.9397e-03,  1.1282e-02,  1.0160e-04,  ..., -6.1987e-04,\n",
      "          2.7268e-03,  3.9505e-03],\n",
      "        [ 5.0857e-03,  7.5302e-03, -1.3297e-03,  ..., -2.0422e-03,\n",
      "          3.5019e-03,  1.4229e-03],\n",
      "        ...,\n",
      "        [-1.4330e-02, -6.2271e-04, -1.8018e-03,  ..., -1.3439e-02,\n",
      "          4.2013e-03,  1.1146e-02],\n",
      "        [ 1.5880e-03,  2.4724e-03,  5.3040e-05,  ..., -3.7138e-03,\n",
      "          3.6048e-03,  1.3661e-03],\n",
      "        [ 1.2945e-03,  1.8769e-03,  2.0955e-04,  ..., -3.7446e-03,\n",
      "          3.7111e-03,  2.1615e-04]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from Dataset_Load import load_dataset #this is an simple lib just for loading some common dataset in torch_geometric\n",
    "from Utils import get_embedding \n",
    "dataname = 'Cora'\n",
    "datadir = '../../../datasets'\n",
    "print('#######---Data----##########')\n",
    "data = load_dataset(dataname, datadir)[0]\n",
    "data = data.cuda() if torch.cuda.is_available() else data\n",
    "\n",
    "z = get_embedding(data.x, data.edge_index, model, num_hop=3, if_rand='False')\n",
    "# data.x means features, data.edge_index means edges\n",
    "# num_hop means after FUG embedding, the representations are propagated on structure num_hop times\n",
    "    # Empirically, for sparser graph dataset (as Cora and Citeseer), num_hop sould be 3 or 5, \n",
    "    # for denser datasets (as Photo and Computers),  num_hop should be 0\n",
    "# if_rand means random sampling or not\n",
    "    # Empirically, for robust outputs, you should shuffle the data when loading rather than here.\n",
    "    # Or not shuffle them just make sure the order of data is randomed (Prevent all sampled data from belonging to the same class). \n",
    "print('#####--Embeddings--#########')\n",
    "print(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
